\documentclass[10pt]{article}

% Packages
\usepackage[margin=0.6in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{enumitem}
\usepackage{titling}

% Compact spacing
\setlength{\parskip}{2pt}
\setlist{nosep, leftmargin=14pt}
\captionsetup{font=small}

% Move the title up
\setlength{\droptitle}{-.65in}

\title{\textbf{Phase 1: Static ASL Alphabet Classification \\Using Transfer Learning with ResNet-18}}
\author{Robin Ede}
\date{November 3, 2025}

\begin{document}

\maketitle
\vspace{-42pt}

\section{Introduction}

This report presents an ablation study on transfer learning strategies for American Sign Language (ASL) alphabet classification. Using the ASL Alphabet dataset (87,000 images across 29 classes), we trained ResNet-18 with four configurations to compare transfer learning approaches against training from scratch. The study addresses key questions about layer freezing strategies, convergence speed, and generalization to real-world conditions.

\section{Methodology}

\textbf{Dataset:} ASL Alphabet (29 classes: A-Z, del, nothing, space). \textbf{Split:} Stratified 80/20 (69,600 train / 17,400 val) with seed=429. \textbf{Training:} 25 epochs, batch size 128, Adam optimizer (lr=0.001, wd=$10^{-4}$). Data augmentation included random flips, rotations, and color jitter. BatchNorm layers kept in eval mode when frozen.

\textbf{Four Configurations:}
\begin{itemize}
    \item \textbf{T-A (Head-Only):} Freeze all layers, train only FC head (0.13\% params)
    \item \textbf{T-B (Layer4+Head):} Freeze stem+layer1-3, train layer4+head (75.13\% params)
    \item \textbf{T-C (Progressive):} Start from T-B checkpoint, unfreeze layer3, train layer3+layer4+head with lr=0.0005 (93.90\% params)
    \item \textbf{S-A (From Scratch):} Random initialization, train all layers (100\% params)
\end{itemize}

\section{Ablation Results on Validation Set}

Table~\ref{tab:val-results} shows validation performance across all four configurations.

\begin{table}[h]
\centering
\caption{Validation set ablation results (model selection based on macro-F1 score).}
\label{tab:val-results}
\vspace{-3pt}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Macro-F1} & \textbf{Accuracy} & \textbf{Best Epoch} & \textbf{Trainable \%} \\
\midrule
T-A (Head Only) & 0.9625 & 96.26\% & 25 & 0.13\% \\
T-B (Layer4+Head) & 0.9999 & 99.99\% & 11 & 75.13\% \\
\textbf{T-C (Progressive)} & \textbf{1.0000} & \textbf{100.00\%} & \textbf{17} & \textbf{93.90\%} \\
S-A (From Scratch) & 0.9998 & 99.98\% & 21 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Model Selection:} \textbf{T-C (Progressive Fine-Tuning)} was selected based on achieving the highest validation macro-F1 score (1.0000). T-C achieves perfect validation performance by progressively unfreezing layers, starting from T-B's strong checkpoint and allowing layer3 to adapt with a reduced learning rate (0.0005).

\subsection{Training and Validation Curves}

Figure~\ref{fig:curves} shows training and validation loss/accuracy curves across all models.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figs/training_curves_comparison.png}
    \caption{Training and validation curves for all four configurations over 25 epochs. T-A plateaus around 96\% validation accuracy. T-B converges rapidly with smooth curves by epoch 11. T-C continues improving beyond T-B, reaching perfect validation. S-A shows more training variance but achieves competitive final performance.}
    \label{fig:curves}
    \vspace{-5pt}
\end{figure}

Figure~\ref{fig:f1} compares validation macro-F1 trajectories, clearly showing T-C reaching F1=1.0 at epoch 17.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.60\textwidth]{figs/val_f1_comparison.png}
    \caption{Validation macro-F1 score evolution. T-C achieves perfect F1=1.0, while T-B and S-A plateau just below (F1$\approx$0.9999). T-A shows a significant performance gap throughout training.}
    \label{fig:f1}
    \vspace{-5pt}
\end{figure}

\section{Test Set Performance}

\subsection{Original Test Set (28 Images)}

The selected T-C model was evaluated on the original Kaggle test set containing 28 images (one per class). Table~\ref{tab:test} shows perfect performance.

\begin{table}[h]
\centering
\caption{T-C model performance on original and custom test sets.}
\label{tab:test}
\vspace{-3pt}
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Accuracy} & \textbf{Macro-F1} \\
\midrule
Original Test Set (28 images) & 100.00\% & 1.0000 \\
Custom Test Set (28 images) & 75.00\% & 0.6724 \\
\bottomrule
\end{tabular}
\vspace{-3pt}
\end{table}

\subsection{Custom Test Set (28 Images)}

We collected a new test set of 28 hand-sign images captured with varying lighting conditions, backgrounds, hand positions, and camera angles compared to the training distribution. Performance dropped to 75.00\% accuracy (F1=0.6724), as shown in Table~\ref{tab:test} and Figure~\ref{fig:custom-cm}.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/phase1_confusion_matrix_T-C_test.png}
        \caption{Confusion matrix on original test set (28 images). Perfect diagonal indicates zero misclassifications.}
        \label{fig:test-cm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/phase1_confusion_matrix_T-C_custom.png}
        \caption{Confusion matrix on custom test set (28 images). Off-diagonal entries reveal widespread misclassifications.}
        \label{fig:custom-cm}
    \end{minipage}
\end{figure}

The 25\% accuracy drop indicates the model learned dataset-specific patterns rather than robust hand-shape features, revealing a significant generalization gap.

\section{Comparison: Transfer Learning vs. From Scratch}

\subsection{Test Accuracy}

Both T-C (transfer learning) and S-A (from scratch) achieved near-perfect validation accuracy (100.00\% vs 99.98\%), demonstrating that the ASL dataset provides sufficient scale (87,000 images) for training from scratch. However, \textbf{T-C achieved strictly higher validation accuracy and perfect macro-F1}, establishing it as the superior model for this task.

\subsection{Convergence Speed}

Transfer learning converged significantly faster. T-B (which serves as T-C's initialization) reached 99.99\% validation accuracy at \textbf{epoch 11}, while S-A required epoch 21 to achieve comparable performance (99.98\%)—a \textbf{48\% reduction in training epochs}. T-C, starting from T-B's checkpoint, reached perfect accuracy at epoch 17 of its progressive fine-tuning phase. As shown in Figure~\ref{fig:curves}, transfer learning models exhibit smoother training curves with less variance, while S-A shows more oscillation in training loss, indicating optimization challenges when learning from random initialization.

\subsection{Generalization}

Both models were evaluated only on the test sets described above. While both achieved high validation accuracy, the performance drop on custom images (75\%) applies equally to the transfer learning approach, indicating this is a dataset-level issue rather than a model architecture issue. The generalization challenge stems from the homogeneous nature of the training data rather than the choice of training strategy.

\subsection{Training Stability and Computational Efficiency}

Transfer learning provides superior training stability and efficiency:
\begin{itemize}
    \item \textbf{Faster Convergence:} T-B reaches 99.99\% accuracy at epoch 11 vs. S-A at epoch 21 (48\% fewer epochs)
    \item \textbf{Training Stability:} Smoother loss curves with less variance (Figure~\ref{fig:curves})
    \item \textbf{Efficiency:} T-C trains 93.90\% of parameters with frozen early layers reducing overhead
\end{itemize}

\subsection{Why Did Training From Scratch Perform Well?}

Despite lacking pretrained weights, S-A achieved competitive 99.98\% validation accuracy for several reasons. First, the dataset provides sufficient scale, with 87,000 training images offering adequate examples for learning robust features from scratch. The perfectly balanced class distribution—each of 29 classes containing exactly 3,000 images—prevented class imbalance issues that often plague from-scratch training. Additionally, hand-shape recognition may require specialized features not well-represented in ImageNet's object-centric training data, potentially reducing the transfer learning advantage for this particular domain.

The training strategy also contributed to S-A's success. Effective regularization through data augmentation (flips, rotations, color jitter) and weight decay ($10^{-4}$) prevented overfitting despite training all 11.2 million parameters. Furthermore, the dataset's consistent characteristics—uniform lighting and plain backgrounds—made the task easier, allowing even random initialization to succeed. However, S-A required 91\% more epochs to converge (epoch 21 vs. epoch 11 for T-B), making transfer learning more practical for efficient development.

\section{Conclusions}

This ablation study demonstrates clear benefits of transfer learning with progressive unfreezing for ASL alphabet classification. T-C (Progressive Fine-Tuning) achieved perfect validation performance (100\% accuracy, F1=1.0000) by progressively unfreezing from T-B's checkpoint with reduced learning rate, establishing it as the superior approach. Transfer learning converged 48\% faster than training from scratch, with T-B reaching 99.99\% accuracy at epoch 11 compared to S-A's epoch 21. Beyond convergence speed, T-C provided computational efficiency by training only 93.90\% of parameters with frozen early layers, while exhibiting smoother loss curves and less variance throughout training. Notably, S-A's competitive 99.98\% accuracy demonstrates that sufficient data scale can overcome the lack of pretrained weights, though at higher computational cost.

T-C provides the optimal balance: perfect accuracy (100\%, F1=1.0), smooth convergence with minimal loss variance, computational efficiency, and 48\% faster convergence than training from scratch. However, a critical limitation emerged during evaluation. Despite perfect validation and test performance (100\%), the model achieved only 75\% accuracy on custom images, revealing that the homogeneous training dataset causes learning of dataset-specific artifacts rather than robust hand-shape features. This 25\% performance gap indicates that \textbf{future work must prioritize diverse data collection} to improve real-world generalization.

For similar ASL classification tasks, we recommend using ResNet-18 with ImageNet pretraining and employing two-stage progressive fine-tuning (T-B then T-C with reduced learning rate). Critically, practitioners should collect diverse test data during development to detect generalization gaps before deployment, as perfect validation performance does not guarantee robust real-world behavior.

\end{document}
